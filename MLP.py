# -*- coding: utf-8 -*-
"""Welcome To Colaboratory

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision.datasets import EMNIST
import torchvision.transforms as transforms
import matplotlib.pyplot as plt

# Define the datasets and dataloaders
batch_size = 900
learning_rate = 0.001
num_epochs = 20

# Load EMNIST dataset
transform = transforms.Compose([
    transforms.Resize((32, 32)),  # Resize the image to 32x32
    transforms.ToTensor(),       # Convert the image to a tensor
    transforms.Normalize((0.1307,), (0.3081,))])
train_dataset = EMNIST(root='./data', split='balanced', train=True, download=True, transform=transform)
test_dataset = EMNIST(root='./data', split='balanced', train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

mapping = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabdefghnqrt"
for images, labels in train_loader:
    # Plot the first 10 images in the batch
    fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 5))
    for i, ax in enumerate(axes.flat):
        ax.imshow(images[i].squeeze(), cmap='gray')
        ax.set_title(mapping[labels[i]])
        ax.axis('off')
    plt.show()
    break

import numpy as np
import itertools
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score

# Define function to plot confusion matrix
def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.figure(figsize=(20, 20))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

# Define MLP model
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes, num_hidden_layers):
        super(MLP, self).__init__()
        self.flatten = nn.Flatten()
        self.hidden_layers = nn.ModuleList()
        for i in range(num_hidden_layers):
            self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, num_classes)
    
    def forward(self, x):
        x = self.flatten(x)
        x = nn.functional.relu(self.fc1(x))
        for hidden_layer in self.hidden_layers:
            x = nn.functional.relu(hidden_layer(x))
        x = self.fc2(x)
        return x

# Initialize models
mlp_model = MLP(input_size=1024, hidden_size=256, num_classes=47, num_hidden_layers=4)

# Define loss function and optimizer
l2_lambda = 0.001
criterion = nn.CrossEntropyLoss()
mlp_optimizer1 = optim.Adam(mlp_model.parameters(), lr=learning_rate, weight_decay = l2_lambda)

# Train MLP model
for optimizer_idx, mlp_optimizer in enumerate([mlp_optimizer1]):

  train_losses = []
  train_accs = []
  test_losses = []
  test_accs = []
  best_acc = 0.0
  for epoch in range(num_epochs):
      mlp_model.train()
      train_loss = 0.0
      train_correct = 0
      train_total = 0
      for i, (images, labels) in enumerate(train_loader):
          images = images
          labels = labels

          # Forward pass
          outputs = mlp_model(images)
          loss = criterion(outputs, labels)
          
          # Backward and optimize
          mlp_optimizer.zero_grad()
          loss.backward()
          mlp_optimizer.step()
          
          # Calculate training loss and accuracy
          train_loss += loss.item()
          _, predicted = torch.max(outputs.data, 1)
          train_total += labels.size(0)
          train_correct += (predicted == labels).sum().item()
          total_steps = len(train_loader) * batch_size

      # Calculate training accuracy and add to list
      train_acc = train_correct / train_total
      train_accs.append(train_acc)
      
      # Calculate test loss and accuracy
      mlp_model.eval()
      test_loss = 0.0
      test_correct = 0
      test_total = 0
      test_predictions = []
      test_targets = []
      test_acc=[]
      with torch.no_grad():
          for images, labels in test_loader:
              images = images
              labels = labels
              
              # Forward pass
              outputs = mlp_model(images)
              loss = criterion(outputs, labels)
              
              # Calculate test loss and accuracy
              test_loss += loss.item()
              _, predicted = torch.max(outputs.data, 1)
              acc = accuracy_score(labels.cpu(), predicted.cpu())
              test_acc.append(acc)
              test_targets.extend(labels.numpy())
              test_predictions.extend(predicted.numpy())
              test_total += labels.size(0)
              test_correct += (predicted == labels).sum().item()

      # Calculate test accuracy and add to list
      test_acc = test_correct / test_total
      test_accs.append(test_acc)
      test_losses.append(test_loss / len(test_loader))
      train_losses.append(train_loss / len(train_loader))
      conf_mat = confusion_matrix(test_targets, test_predictions)
      
      precision = precision_score(test_targets, test_predictions, average = 'macro')
      recall = recall_score(test_targets, test_predictions, average = 'macro')
      f1 = f1_score(test_targets, test_predictions, average = 'macro')

      # Save the model with the best test accuracy
      if test_acc > best_acc:
          torch.save(mlp_model.state_dict(), 'best_mlp_model.pt')
          best_acc = test_acc
      
      # Print epoch results
      train_loss /= len(train_loader)
      print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Loss: {test_loss/len(test_loader):.4f}, Test Acc: {test_acc:.4f}')
  print(f'Precision: {precision:.4f} \n Recall: {recall:.4f}\n F1 score: {f1:.4f}\n')
  test_acc = np.mean(test_acc)
  print(f"Overall Accuracy: {test_acc}")
  # Plot confusion matrix
  classes = [str(i) for i in range(47)]
  plot_confusion_matrix(conf_mat, classes=classes, normalize=True, cmap=plt.cm.Greens)
  plt.show()
  # Plot the loss and accuracy curves
  plt.plot(train_losses, label='Train Loss')
  plt.plot(test_losses, label='Test Loss')
  plt.legend()
  plt.show()

  plt.plot(train_accs, label='Train Acc')
  plt.plot(test_accs, label='Test Acc')
  plt.legend()
  plt.show()